#!/usr/bin/env python3
"""
æ³Šæ¾ç¼–ç æ—¶é—´æ­¥é•¿ä¼˜åŒ–æµ‹è¯•
æµ‹è¯•ä¸åŒæ—¶é—´æ­¥é•¿ä¸‹çš„ç¼–ç -è§£ç æ•ˆæœ
"""

import torch
import numpy as np
import matplotlib.pyplot as plt

def simple_poisson_encode(x):
    """
    ç®€åŒ–çš„æ³Šæ¾ç¼–ç ï¼šä½¿ç”¨å‡åŒ€éšæœºæ•°
    x: è¾“å…¥æ¦‚ç‡å¼ é‡ [0,1]
    è¿”å›: äºŒè¿›åˆ¶è„‰å†²å¼ é‡ {0,1}
    """
    return torch.rand_like(x).le(x).to(x)

def simple_poisson_decode(spike_sequence, method='mean'):
    """
    ç®€åŒ–çš„æ³Šæ¾è§£ç ï¼šä»è„‰å†²åºåˆ—æ¢å¤ç‰¹å¾å€¼
    
    Args:
        spike_sequence: è„‰å†²åºåˆ— (n_features, time_steps) æˆ– (time_steps,)
        method: è§£ç æ–¹æ³• ('mean', 'median', 'mode')
    
    Returns:
        decoded_features: è§£ç åçš„ç‰¹å¾å€¼ [0,1]
    """
    if spike_sequence.dim() == 1:
        # å•ä¸ªç‰¹å¾çš„æƒ…å†µ
        if method == 'mean':
            return spike_sequence.float().mean()
        elif method == 'median':
            return spike_sequence.float().median().values
        elif method == 'mode':
            return spike_sequence.float().mode().values
    else:
        # å¤šä¸ªç‰¹å¾çš„æƒ…å†µ
        if method == 'mean':
            return spike_sequence.float().mean(dim=1)  # æ²¿æ—¶é—´ç»´åº¦å¹³å‡
        elif method == 'median':
            return spike_sequence.float().median(dim=1).values
        elif method == 'mode':
            return spike_sequence.float().mode(dim=1).values

def test_single_timestep(features, time_steps, seed=42, method='mean'):
    """
    æµ‹è¯•å•ä¸ªæ—¶é—´æ­¥é•¿çš„ç¼–ç -è§£ç æ•ˆæœ
    
    Args:
        features: åŸå§‹ç‰¹å¾å€¼ tensor [0,1]
        time_steps: æ—¶é—´æ­¥é•¿
        seed: éšæœºç§å­
        method: è§£ç æ–¹æ³•
    
    Returns:
        dict: åŒ…å«åŸå§‹å€¼ã€é‡æ„å€¼ã€è¯¯å·®ç­‰ä¿¡æ¯
    """
    torch.manual_seed(seed)
    
    # æ‰©å±•åˆ°æ—¶é—´ç»´åº¦ (n_features, time_steps)
    if features.dim() == 1:
        expanded_features = features.unsqueeze(1).repeat(1, time_steps)
    else:
        expanded_features = features.repeat(1, time_steps)
    
    # ç¼–ç 
    spikes = simple_poisson_encode(expanded_features)
    
    # è§£ç 
    reconstructed = simple_poisson_decode(spikes, method=method)
    
    # è®¡ç®—è¯¯å·®
    mse = torch.mean((features - reconstructed) ** 2).item()
    mae = torch.mean(torch.abs(features - reconstructed)).item()
    
    return {
        'original': features,
        'reconstructed': reconstructed,
        'spikes': spikes,
        'mse': mse,
        'mae': mae,
        'time_steps': time_steps,
        'total_spikes': spikes.sum().item()
    }

def find_optimal_timesteps(features, timestep_range, seed=42, method='mean'):
    """
    å¯»æ‰¾æœ€ä¼˜æ—¶é—´æ­¥é•¿
    
    Args:
        features: æµ‹è¯•ç‰¹å¾å€¼
        timestep_range: æ—¶é—´æ­¥é•¿èŒƒå›´ (list)
        seed: éšæœºç§å­
        method: è§£ç æ–¹æ³•
    
    Returns:
        dict: å„æ—¶é—´æ­¥é•¿çš„æµ‹è¯•ç»“æœ
    """
    print("ğŸ” å¯»æ‰¾æœ€ä¼˜æ—¶é—´æ­¥é•¿...")
    print("=" * 60)
    print(f"{'æ—¶é—´æ­¥é•¿':>8} {'MSE':>10} {'MAE':>10} {'æ€»è„‰å†²':>8} {'æ•ˆç‡':>8}")
    print("-" * 60)
    
    results = {}
    
    for ts in timestep_range:
        result = test_single_timestep(features, ts, seed=seed, method=method)
        efficiency = result['total_spikes'] / (len(features) * ts)  # è„‰å†²æ•ˆç‡
        
        results[ts] = result
        results[ts]['efficiency'] = efficiency
        
        print(f"{ts:>8} {result['mse']:>10.6f} {result['mae']:>10.6f} "
              f"{int(result['total_spikes']):>8} {efficiency:>8.3f}")
    
    # æ‰¾å‡ºæœ€ä½³æ—¶é—´æ­¥é•¿
    best_ts_mse = min(results.keys(), key=lambda k: results[k]['mse'])
    best_ts_mae = min(results.keys(), key=lambda k: results[k]['mae'])
    
    print("-" * 60)
    print(f"ğŸ† æœ€ä½³æ—¶é—´æ­¥é•¿ (MSE): {best_ts_mse} (MSE: {results[best_ts_mse]['mse']:.6f})")
    print(f"ğŸ† æœ€ä½³æ—¶é—´æ­¥é•¿ (MAE): {best_ts_mae} (MAE: {results[best_ts_mae]['mae']:.6f})")
    
    return results

# def visualize_encoding_quality(features, timestep_range, seed=42):
#     """
#     å¯è§†åŒ–ç¼–ç è´¨é‡éšæ—¶é—´æ­¥é•¿çš„å˜åŒ–
#     """
#     results = find_optimal_timesteps(features, timestep_range, seed=seed)
    
#     # æå–æ•°æ®
#     timesteps = list(results.keys())
#     mse_values = [results[ts]['mse'] for ts in timesteps]
#     mae_values = [results[ts]['mae'] for ts in timesteps]
#     efficiency_values = [results[ts]['efficiency'] for ts in timesteps]
    
#     # ç»˜å›¾
#     fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))
    
#     # MSE vs æ—¶é—´æ­¥é•¿
#     ax1.plot(timesteps, mse_values, 'b-o', linewidth=2, markersize=6)
#     ax1.set_xlabel('æ—¶é—´æ­¥é•¿')
#     ax1.set_ylabel('MSE')
#     ax1.set_title('å‡æ–¹è¯¯å·® vs æ—¶é—´æ­¥é•¿')
#     ax1.grid(True, alpha=0.3)
    
#     # MAE vs æ—¶é—´æ­¥é•¿
#     ax2.plot(timesteps, mae_values, 'r-s', linewidth=2, markersize=6)
#     ax2.set_xlabel('æ—¶é—´æ­¥é•¿')
#     ax2.set_ylabel('MAE')
#     ax2.set_title('å¹³å‡ç»å¯¹è¯¯å·® vs æ—¶é—´æ­¥é•¿')
#     ax2.grid(True, alpha=0.3)
    
#     # è„‰å†²æ•ˆç‡ vs æ—¶é—´æ­¥é•¿
#     ax3.plot(timesteps, efficiency_values, 'g-^', linewidth=2, markersize=6)
#     ax3.set_xlabel('æ—¶é—´æ­¥é•¿')
#     ax3.set_ylabel('è„‰å†²æ•ˆç‡')
#     ax3.set_title('è„‰å†²æ•ˆç‡ vs æ—¶é—´æ­¥é•¿')
#     ax3.grid(True, alpha=0.3)
    
#     # åŸå§‹ vs é‡æ„å¯¹æ¯” (é€‰æ‹©ä¸­ç­‰æ—¶é—´æ­¥é•¿)
#     mid_ts = timesteps[len(timesteps)//2]
#     result = results[mid_ts]
#     ax4.scatter(result['original'].numpy(), result['reconstructed'].numpy(), 
#                alpha=0.7, s=50)
#     ax4.plot([0, 1], [0, 1], 'r--', linewidth=2, label='ç†æƒ³çº¿')
#     ax4.set_xlabel('åŸå§‹ç‰¹å¾å€¼')
#     ax4.set_ylabel('é‡æ„ç‰¹å¾å€¼')
#     ax4.set_title(f'åŸå§‹ vs é‡æ„ (æ—¶é—´æ­¥é•¿={mid_ts})')
#     ax4.legend()
#     ax4.grid(True, alpha=0.3)
    
#     plt.tight_layout()
#     plt.show()
    
#     return results

def visualize_encoding_quality(features, timestep_range, seed=42):
    """
    å¯è§†åŒ–ç¼–ç è´¨é‡éšæ—¶é—´æ­¥é•¿çš„å˜åŒ–
    """
    results = find_optimal_timesteps(features, timestep_range, seed=seed)
    
    # æå–æ•°æ®
    timesteps = list(results.keys())
    mse_values = [results[ts]['mse'] for ts in timesteps]
    mae_values = [results[ts]['mae'] for ts in timesteps]
    efficiency_values = [results[ts]['efficiency'] for ts in timesteps]
    
    # ç»˜å›¾
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(12, 10))
    
    # MSE vs æ—¶é—´æ­¥é•¿
    ax1.plot(timesteps, mse_values, 'b-o', linewidth=2, markersize=6)
    ax1.set_xlabel('Time Steps')
    ax1.set_ylabel('MSE')
    ax1.set_title('Mean Squared Error vs Time Steps')
    ax1.grid(True, alpha=0.3)
    
    # MAE vs æ—¶é—´æ­¥é•¿
    ax2.plot(timesteps, mae_values, 'r-s', linewidth=2, markersize=6)
    ax2.set_xlabel('Time Steps')
    ax2.set_ylabel('MAE')
    ax2.set_title('Mean Absolute Error vs Time Steps')
    ax2.grid(True, alpha=0.3)
    
    # è„‰å†²æ•ˆç‡ vs æ—¶é—´æ­¥é•¿
    ax3.plot(timesteps, efficiency_values, 'g-^', linewidth=2, markersize=6)
    ax3.set_xlabel('Time Steps')
    ax3.set_ylabel('Spike Efficiency')
    ax3.set_title('Spike Efficiency vs Time Steps')
    ax3.grid(True, alpha=0.3)
    
    # åŸå§‹ vs é‡æ„å¯¹æ¯” (é€‰æ‹©ä¸­ç­‰æ—¶é—´æ­¥é•¿)
    mid_ts = timesteps[len(timesteps)//2]
    result = results[mid_ts]
    ax4.scatter(result['original'].numpy(), result['reconstructed'].numpy(), 
               alpha=0.7, s=50)
    ax4.plot([0, 1], [0, 1], 'r--', linewidth=2, label='Ideal Line')
    ax4.set_xlabel('Original Features')
    ax4.set_ylabel('Reconstructed Features')
    ax4.set_title(f'Original vs Reconstructed (Time Steps={mid_ts})')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return results

def demo_timestep_optimization():
    """
    æ¼”ç¤ºæ—¶é—´æ­¥é•¿ä¼˜åŒ–è¿‡ç¨‹
    """
    print("ğŸ¯ æ³Šæ¾ç¼–ç æ—¶é—´æ­¥é•¿ä¼˜åŒ–æ¼”ç¤º")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•ç‰¹å¾
    torch.manual_seed(42)
    # features = torch.tensor([0.1, 0.2, 0.3, 0.5, 0.7, 0.8, 0.9])
    features = torch.tensor([0.05, 0.5, 0.6, 0.48, 0.49, 0.50, 0.51, 0.52, 0.53, 0.54, 0.55, 0.95])
    print(f"æµ‹è¯•ç‰¹å¾: {features.tolist()}")
    
    # æµ‹è¯•ä¸åŒæ—¶é—´æ­¥é•¿
    # timestep_range = [5, 10, 20, 50, 100, 200, 500, 1000]
    timestep_range = [x for x in range(1,31)]
    
    # å¯»æ‰¾æœ€ä¼˜æ—¶é—´æ­¥é•¿
    results = find_optimal_timesteps(features, timestep_range, seed=42)
    
    # è¯¦ç»†åˆ†æå‡ ä¸ªå…³é”®æ—¶é—´æ­¥é•¿
    print("\n" + "=" * 60)
    print("è¯¦ç»†åˆ†æ")
    print("=" * 60)
    
    key_timesteps = [10, 50, 200]
    for ts in key_timesteps:
        if ts in results:
            result = results[ts]
            print(f"\næ—¶é—´æ­¥é•¿ {ts}:")
            print(f"  åŸå§‹ç‰¹å¾: {result['original'].tolist()}")
            print(f"  é‡æ„ç‰¹å¾: {result['reconstructed'].tolist()}")
            print(f"  MSE: {result['mse']:.6f}")
            print(f"  MAE: {result['mae']:.6f}")
            print(f"  è„‰å†²æ•ˆç‡: {result['efficiency']:.3f}")
    
    # å¯è§†åŒ–
    print(f"\nğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    visualize_encoding_quality(features, timestep_range, seed=42)
    
    return results

if __name__ == "__main__":
    demo_timestep_optimization()